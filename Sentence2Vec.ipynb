{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An baseline python3 implementation of \"A Simple But Tough to Beat Baseline for Sentence Embeddngs\" by Sanjeev Arora, Yingyu Liang, Tengyu Ma. \n",
    "\n",
    "Significantly modified from https://github.com/peter3125/sentence2vec.git, which is under the Apache license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a method for quickly deriving sentence embeddings by simpy taking the weighted average of individual word embeddings in a sentence and normalizing them by a 'smooth inverse frequency' (SIF). This SIF is a function of the constant `a` parameter and each word's individual frequency.\n",
    "\n",
    "In this way, we don't have to encode sentence embeddings directly, a process which would be a lot more expensive than encoding word embeddings. After Arora et al., if you've already got a serialized word vector model, you also already have a sentence embedding model.\n",
    "\n",
    "It's not much more work to create a framework to compare the similarity of an input sentence to a collection of other sentences in a corpus. For our purposes we're comparing an input utterance against a set of questions. The most similar question in this set of questions is matched to a corresponding answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and load english language model and word vector model\n",
    "from __future__ import print_function\n",
    "import time\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import spacy\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity # <<< this isn't being used!\n",
    "from typing import List\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "gnews_model = gensim.models.KeyedVectors.load_word2vec_format('~/Downloads/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create `Word` object with `text` and `vector` attributes. A `Sentence` object is merely a list of `Word` objects, with some additional functions for exposing its orthographic representation and length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, text, vector):\n",
    "        self.text = text\n",
    "        self.vector = vector\n",
    "\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, word_list):\n",
    "        self.word_list = word_list\n",
    "    # return the length of a sentence\n",
    "    def len(self):\n",
    "        return(len(self.word_list))\n",
    "    def __str__(self):\n",
    "        word_str_list = [word.text for word in self.word_list]\n",
    "        return ' '.join(word_str_list)\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function takes a list of strings, and converts them into a list of `Sentence` objects (themselves lists of `Words`, which are just tuples of `text`/`vector` data). This format is necessary to pass sentences to the `sentence2vec` algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preloading_sentences(sentence_list, model):\n",
    "    \"\"\"\n",
    "    Converts a list of sentences into a list of Sentence (and Word) objects\n",
    "\n",
    "    input: a list of sentences, embedding_size\n",
    "    output: a list of Sentence objects, containing Word objects, which contain 'text' and word vector attributes\n",
    "    \"\"\"\n",
    "    embedding_size = 300\n",
    "    all_sent_info = []\n",
    "    for sentence in sentence_list:\n",
    "        sent_info = []\n",
    "        spacy_sentence = nlp(sentence)\n",
    "        for word in spacy_sentence:\n",
    "            if word.text in model.vocab:\n",
    "                sent_info.append(Word(word.text, model[word.text]))\n",
    "        # todo: if sent_info > 0, append, else don't\n",
    "        all_sent_info.append(Sentence(sent_info))\n",
    "    return(all_sent_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use some proxy for word frequency, in order to find the smooth inverse frequency. Here we're using the count from the Google News word2vec model. Note that this model doesn't accurately represent our own financial services domain text - but it still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency(word_text, vec_model):\n",
    "    wf = vec_model.vocab[word_text].count\n",
    "    return(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main sentence 'vectorization' function. Except all we're really doing here is iterating over words in sentences and building a list of word vector lists. Then we normalize it by a 'smooth inverse frequency', and take its first principal component. We're also padding our sentences with a lot of zeroes to fit them to the size of the embedding model. By using our own word vector model, this should be a lot more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vec(sentence_list, embedding_size, a=1e-3):\n",
    "    \"\"\"\n",
    "    A SIMPLE BUT TOUGH TO BEAT BASELINE FOR SENTENCE EMBEDDINGS\n",
    "\n",
    "    Sanjeev Arora, Yingyu Liang, Tengyu Ma\n",
    "    Princeton University\n",
    "    \"\"\"\n",
    "    sentence_set = [] # intermediary list of sentence vectors before PCA\n",
    "    sent_list = [] # return list of input sentences in the output\n",
    "    for sentence in sentence_list:\n",
    "        this_sent = []\n",
    "        vs = np.zeros(embedding_size) # add all w2v values into one vector for the sentence\n",
    "        sentence_length = sentence.len()\n",
    "        for word in sentence.word_list:\n",
    "            this_sent.append(word.text)\n",
    "            word_freq = get_word_frequency(word.text, gnews_model)\n",
    "            a_value = a / (a + word_freq) # smooth inverse frequency, SIF\n",
    "            vs = np.add(vs, np.multiply(a_value, word.vector)) # vs += sif * word_vector\n",
    "        vs = np.divide(vs, sentence_length) # weighted average, normalized by sentence length\n",
    "        sentence_set.append(vs) # add to our existing re-caculated set of sentences\n",
    "        sent_list.append(' '.join(this_sent))\n",
    "    # calculate PCA of this sentence set\n",
    "    pca = PCA(n_components=embedding_size)\n",
    "    pca.fit(np.array(sentence_set))\n",
    "    u = pca.components_[0]  # the PCA vector\n",
    "    u = np.multiply(u, np.transpose(u))  # u x uT\n",
    "    # pad the vector? (occurs if we have less sentences than embeddings_size)\n",
    "    if len(u) < embedding_size:\n",
    "        for i in range(embedding_size - len(u)):\n",
    "            u = np.append(u, 0)\n",
    "    # resulting sentence vectors, vs = vs -u * uT * vs\n",
    "    sentence_vecs = []\n",
    "    for vs in sentence_set:\n",
    "        sub = np.multiply(u, vs)\n",
    "        sentence_vecs.append(np.subtract(vs, sub))\n",
    "    return(sentence_vecs, sent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads a list of sentences and returns a list of sentence vectors and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sen_embeddings(sentence_list):\n",
    "    \"\"\"\n",
    "    Create Sentence and Word objects from a list and pass them to sentence_to_vec()\n",
    "    Return a list of _sentence embeddings_ for all sentences in the list\n",
    "    \"\"\"\n",
    "    embedding_size = 300\n",
    "    all_sent_info = preloading_sentences(sentence_list, gnews_model)\n",
    "    sentence_vectors, sent_list = sentence_to_vec(all_sent_info, embedding_size)\n",
    "    return(sentence_vectors, sent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `sklearn` function takes a list of lists (for each sentence, we have a list of sentence embeddings reprepsenting their relative positions in non-Euclidean space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_distance(sentence_list):\n",
    "    \"\"\"\n",
    "    Create Sentence and Word objects from a list and pass them to sentence_to_vec()\n",
    "    Return a matrix of the _cosine distance_ of elements in the matrix\n",
    "    This is used for sentence similarity functions\n",
    "\n",
    "    input: A list of plaintext sentences\n",
    "    output: A list of sentence distances\n",
    "    \"\"\"\n",
    "    sentence_vectors, sent_list = get_sen_embeddings(sentence_list)\n",
    "    cos_list = cosine_similarity(sentence_vectors, Y=None, dense_output=True)\n",
    "    return(cos_list, sent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple function just looks up the cosine similarity matrix for the last sentence in the list (this is the input sentence), and returns the index of the most similiar question sentence to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(utterance, sentence_list):\n",
    "    \"\"\"\n",
    "    Takes an input utterance and a corpus sentence_list to compare it to,\n",
    "    and returns a dict of the utterance, closest question, and relevant answer\n",
    "    \"\"\"\n",
    "    sentence_list.append(utterance)\n",
    "    cos_list, sent_text = get_cos_distance(sentence_list)\n",
    "    # check out the similarity matrix for the utterance\n",
    "    tmp_list = list(cos_list[len(cos_list)-1])\n",
    "    # get the index of the question with the highest simliarity\n",
    "    tmp_indx = tmp_list.index(max(tmp_list[0:len(tmp_list)-2]))\n",
    "    return(tmp_indx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interactive demo reads an input `utterance`, appends it to the list of questions. Then it builds a sentence embedding model of this larger list, derives cosine distances between all sentences, and returns the index of the closest question to the input utterance. Then we print the question itself, and its corresponding answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance = \"I have a shortage :(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What's a shortage?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faq_csv = pd.read_csv(\"./utterances_test.csv\")\n",
    "sentence_list = list(faq_csv['utterance'])\n",
    "answer_list = list(faq_csv['answer'])\n",
    "max_idx = get_most_similar(utterance, sentence_list)\n",
    "sentence_list[max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If the funds in your escrow account are projected to be below your minimum balance at the lowest point in the 12-month period, you have a shortage.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_list[max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
